{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of msft AI challenge",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p56lKoZTJHdZ",
        "colab_type": "code",
        "outputId": "4d95c762-bdef-438f-8728-c852bde2e47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "# # Run this only once on a machine\n",
        "\n",
        "# !pip install pytorch-pretrained-bert\n",
        "# !pip install livelossplot\n",
        "# !wget \"https://competitions.codalab.org/my/datasets/download/69a3e8d0-b836-48b8-8795-36a6865a1c04\"\n",
        "# !unzip 69a3e8d0-b836-48b8-8795-36a6865a1c04\n",
        "# !rm 69a3e8d0-b836-48b8-8795-36a6865a1c04\n",
        "# !ls -lh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/68/84de54aea460eb5b2e90bf47a429aacc1ce97ff052ec40874ea38ae2331d/pytorch_pretrained_bert-0.4.0-py3-none-any.whl (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Collecting torch>=0.4.1 (from pytorch-pretrained-bert)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    13% |████▏                           | 77.6MB 24.3MB/s eta 0:00:22"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a8rLu17HfIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7_VbyfsNfmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1 = drive.CreateFile({'title': 'best_val.bin'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aUs1tzgKS_I",
        "colab_type": "code",
        "outputId": "df8cb71c-2406-4827-86b8-3ea6ea1ff789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn as nn\n",
        "from livelossplot import PlotLosses\n",
        "import random\n",
        "import csv\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPn8attwqoE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class BertSequence(nn.Module):\n",
        "  \n",
        "#   def __init__(self, config, num_labels):\n",
        "#     super(BertSequence, self).__init__()\n",
        "#     self.config = config\n",
        "#     self.num_labels = num_labels\n",
        "#     self.bert = BertModel(config)\n",
        "#     self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "#     self.classifier = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
        "#                                     nn.ReLU(),\n",
        "#                                     nn.Linear(config.hidden_size, self.num_labels))\n",
        "#     for module in self.modules():\n",
        "#       if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "#               module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#       elif isinstance(module, BertLayerNorm):\n",
        "#           module.bias.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#           module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#       if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "#           module.bias.data.zero_()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDJ3PTVq70qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertSequence(nn.Module):\n",
        "  \n",
        "  def __init__(self, config, num_labels):\n",
        "    \n",
        "    # configs :\n",
        "    # bert-base-uncased: 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    # bert-large-uncased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    # bert-base-cased: 12-layer, 768-hidden, 12-heads , 110M parameters\n",
        "    # bert-large-cased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    \n",
        "    super(BertSequence, self).__init__()\n",
        "    \n",
        "    self.hidden = 768\n",
        "    self.num_labels = num_labels\n",
        "    self.config = config\n",
        "    self.bert = BertForSequenceClassification.from_pretrained(config, num_labels = self.hidden)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.layer = nn.Linear(self.hidden, self.num_labels)\n",
        "    self.layer.weight.data.normal_(mean=0.0, std=0.02)\n",
        "    self.layer.bias.data.zero_()\n",
        "    \n",
        "  def forward(self, input_ids, segment_ids, input_mask):\n",
        "    \n",
        "    out = self.bert(input_ids, segment_ids, input_mask)\n",
        "    out = self.relu(out)\n",
        "    out = self.layer(out)\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joBV4vt8RKwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "gradient_accumulation_steps = 4\n",
        "num_train_epochs = 2000\n",
        "lr = 2e-2\n",
        "max_seq_length = 256\n",
        "warmup_proportion = 0.1 \n",
        "# Proportion of training to perform linear learning rate warmup for. \"\"E.g., 0.1 = 10% of training.\")\n",
        "lr_chng_iter = 10000\n",
        "eval_iter = 5\n",
        "val_split = 0.2\n",
        "best_val = 0\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiDpKh2ARNGw",
        "colab_type": "code",
        "outputId": "f9f92163-f4ba-41d1-d344-9de829e61579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lines = []\n",
        "\n",
        "num_examples = 500\n",
        "\n",
        "with open('data.tsv', \"r\", encoding='utf-8') as f:\n",
        "  reader = csv.reader(f, delimiter=\"\\t\")\n",
        "  for line in reader:\n",
        "    \n",
        "      num_examples-=1\n",
        "      if num_examples < 0:\n",
        "        break\n",
        "        \n",
        "      lines.append(line)\n",
        "      \n",
        "\n",
        "''' USING 10k examples only for testing !!!'''\n",
        "\n",
        "# Punctuation etc. stuff is handled by tokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' USING 10k examples only for testing !!!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjIHavKmfTzs",
        "colab_type": "code",
        "outputId": "181ae7bd-6c3c-40aa-b0c0-d92b0bb25847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "random.shuffle(lines)\n",
        "train_lines = lines[:int((1-val_split)*len(lines))]\n",
        "val_lines = lines[int((1-val_split)*len(lines)):]\n",
        "print('Train examples = ', len(train_lines))\n",
        "print('Val examples = ', len(val_lines))\n",
        "del lines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train examples =  400\n",
            "Val examples =  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VfHXcFXh79x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrMb4srqiG2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples = [InputExample(guid = line[0], text_a = line[1], text_b = line[2], label = line[3]) for line in train_lines]\n",
        "val_examples = [InputExample(guid = line[0], text_a = line[1], text_b = line[2], label = line[3]) for line in val_lines]\n",
        "del train_lines\n",
        "del val_lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCY6AxaMh3l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWRl2TrQJi11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXwUyEQijTlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFBsU60LcbY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        \n",
        "        if ex_index % 1000 == 0:\n",
        "          print('{} examples done out of {}'.format(ex_index, len(examples)))\n",
        "        \n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:7\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        label_id = label_map[int(example.label)]\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_id))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf5Ug5_RWPxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, labels):\n",
        "    outputs = np.argmax(out, axis=1)\n",
        "    \n",
        "#     correct_one = np.sum(np.where(outputs == 1)[0] == np.where(labels == 1)) #count of number of times the model predicted 1 and the label is also 1\n",
        "    correct_one = np.sum((outputs == labels) * outputs)\n",
        "    precision =  correct_one/np.sum(outputs == 1)\n",
        "    recall = correct_one/np.sum(labels==1)\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "#     print(\"f1: \", f1, \" precision:\", precision, \" recall:\", recall)\n",
        "#     print(\"labels:\", labels)\n",
        "#     print(\"out:\", out)\n",
        "    \n",
        "    return [np.sum(outputs == labels), f1, precision, recall, correct_one]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9va6BEdShXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hiO2xsrJi0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertSequence('bert-base-uncased',num_labels = 2).cuda()\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "num_train_steps = int(len(train_examples)/batch_size) # steps in 1 epoch\n",
        "t_total = num_train_steps*num_train_epochs # total number of steps in training\n",
        "optimizer = BertAdam(optimizer_grouped_parameters, lr=lr, warmup=warmup_proportion, t_total=t_total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SudkAimBAVft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.backends.cudnn.benchmark = True    # would speed up runtime hopefully\n",
        "label_list = [0,1]         # label map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHbsjkLdJixD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = convert_examples_to_features(train_examples, label_list, max_seq_length, tokenizer)\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "del train_features\n",
        "_ = model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu4t5LefGn2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_features = convert_examples_to_features(val_examples, label_list, max_seq_length, tokenizer)\n",
        "all_input_ids = torch.tensor([f.input_ids for f in val_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in val_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in val_features], dtype=torch.long)\n",
        "all_label_ids = torch.tensor([f.label_id for f in val_features], dtype=torch.long)\n",
        "val_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioSYr1nEeZcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model(model, epoch, iteration, loss, label_list, max_seq_length, loss_fct):\n",
        "  \n",
        "  global best_val\n",
        "  global gradient_accumulation_steps\n",
        "  global file1 \n",
        "  \n",
        "  _ = model.eval()\n",
        "  \n",
        "  eval_loss, eval_accuracy, f1, precision, recall = 0, 0, 0, 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  \n",
        "  for input_ids, input_mask, segment_ids, label_ids in val_dataloader:\n",
        "    input_ids = input_ids.cuda()\n",
        "    input_mask = input_mask.cuda()\n",
        "    segment_ids = segment_ids.cuda()\n",
        "    label_ids = label_ids.cuda()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, segment_ids, input_mask)\n",
        "\n",
        "    tmp_eval_accuracy, tmp_f1, tmp_precision, tmp_recall, tmp_correct_one = tuple(accuracy(logits.detach().cpu().numpy(), label_ids.cpu().numpy()))    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    f1 += tmp_f1\n",
        "    precision += tmp_precision\n",
        "    recall += tmp_recall\n",
        "    \n",
        "    tmp_eval_loss = loss_fct(logits.view(-1, 2), label_ids.view(-1))\n",
        "    tmp_eval_loss /= gradient_accumulation_steps\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    \n",
        "    nb_eval_examples += input_ids.size(0)\n",
        "    nb_eval_steps += 1\n",
        "    \n",
        "  eval_loss = eval_loss / nb_eval_steps\n",
        "  eval_accuracy = eval_accuracy / nb_eval_examples\n",
        "  f1 /= nb_eval_steps\n",
        "  precision /= nb_eval_steps\n",
        "  recall /= nb_eval_steps\n",
        "\n",
        "  if eval_accuracy > best_val:\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "    output_model_file = \"best_val.bin\"\n",
        "    torch.save({'stat_dict': model_to_save.state_dict(), \n",
        "                'eval_loss' : eval_loss,\n",
        "                'eval_accuracy' : eval_accuracy,\n",
        "                'f1' : f1,\n",
        "                'precision' : precision,\n",
        "                'recall' : recall}\n",
        "                , output_model_file)\n",
        "\n",
        "    file1.SetContentFile('best_val.bin')\n",
        "    file1.Upload()\n",
        "    print('Best eval_accuracy = ', eval_accuracy, ' f1 = ', f1, ' precision = ', precision, ' recall = ', recall)\n",
        "      \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2120f2qeGFiA",
        "colab": {}
      },
      "source": [
        "weights = [0.15, 0.85]\n",
        "class_weights = torch.FloatTensor(weights).cuda()\n",
        "loss_fct = CrossEntropyLoss(weight = class_weights)\n",
        "\n",
        "total_step = 0\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "  print('epoch = ', epoch)\n",
        "  for iteration, batch in enumerate(train_dataloader):\n",
        "    input_ids, input_mask, segment_ids, label_ids = batch\n",
        "    input_ids = input_ids.cuda()\n",
        "    input_mask = input_mask.cuda()\n",
        "    segment_ids = segment_ids.cuda()\n",
        "    label_ids = label_ids.cuda()\n",
        "    \n",
        "    logits = model(input_ids, segment_ids, input_mask)\n",
        "    \n",
        "    train_accuracy_params = accuracy(logits.detach().cpu().numpy(), label_ids.cpu().numpy())\n",
        "    train_accuracy = train_accuracy_params[0]\n",
        "    f1 = train_accuracy_params[1]\n",
        "    precision = train_accuracy_params[2]\n",
        "    recall = train_accuracy_params[3]\n",
        "    correct_one = train_accuracy_params[4]\n",
        "    \n",
        "    loss = loss_fct(logits.view(-1, 2), label_ids.view(-1))\n",
        "    loss = loss / gradient_accumulation_steps\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    if (iteration + 1) % gradient_accumulation_steps == 0:\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "    if iteration % lr_chng_iter == 0:\n",
        "      new_lr = lr * warmup_linear(total_step/t_total, warmup_proportion)\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = new_lr\n",
        "    \n",
        "    total_step += 1\n",
        "    \n",
        "    if iteration % 10 == 0:\n",
        "      print(\"iteration:\", iteration, \" loss:\", loss.item(), \"train_accuracy:\", train_accuracy, \" f1:\", f1, \" precision:\", precision, \" recall:\", recall, \" correct_one:\", correct_one)\n",
        "    \n",
        "    if (iteration + 1) % eval_iter == 0:\n",
        "        eval_model(model, epoch, iteration, loss, label_list, max_seq_length, loss_fct)\n",
        "        _ = model.train()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQUfnHfAKwUA",
        "colab_type": "code",
        "outputId": "cbc9b122-b07e-41c1-a495-b4c86adeadf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 26 06:35:11 2018       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekCn7UqaJind",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loaa9aNVJiko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQXDQhZlJihe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_temp  = logits.detach().cpu().numpy()\n",
        "out_temp = np.argmax(out_temp, axis=1)\n",
        "out_temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGioQGwh01pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.where(out_temp==0)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpFbaU1i1Vz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8zRVDxP1e0e",
        "colab_type": "code",
        "outputId": "ed168714-10c0-4913-c598-0d6c78eac5b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "for p,n in zip(model.parameters(),model._all_weights[0]):\n",
        "    if n[:6] == 'weight':\n",
        "        print('===========\\ngradient:{}\\n----------\\n{}'.format(n,p.grad))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-7ae4eae76225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'===========\\ngradient:{}\\n----------\\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 535\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute '_all_weights'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmGOJZGD-lBP",
        "colab_type": "code",
        "outputId": "3c4ca709-bf8e-4f34-ae3c-17d8b2bd45bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.parameters == model.parameters"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDKErR_WBQ3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}