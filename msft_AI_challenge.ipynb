{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "msft AI challenge",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p56lKoZTJHdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Run this only once on a machine\n",
        "\n",
        "# !pip install pytorch-pretrained-bert\n",
        "# !pip install livelossplot\n",
        "# !wget \"https://competitions.codalab.org/my/datasets/download/69a3e8d0-b836-48b8-8795-36a6865a1c04\"\n",
        "# !unzip 69a3e8d0-b836-48b8-8795-36a6865a1c04\n",
        "# !rm 69a3e8d0-b836-48b8-8795-36a6865a1c04\n",
        "# !ls -lh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a8rLu17HfIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7_VbyfsNfmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1 = drive.CreateFile({'title': 'best_val.bin'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aUs1tzgKS_I",
        "colab_type": "code",
        "outputId": "1e5641d6-29a9-4bc4-c9ec-c744b1058bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn as nn\n",
        "from livelossplot import PlotLosses\n",
        "import random\n",
        "import csv\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPn8attwqoE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class BertSequence(nn.Module):\n",
        "  \n",
        "#   def __init__(self, config, num_labels):\n",
        "#     super(BertSequence, self).__init__()\n",
        "#     self.config = config\n",
        "#     self.num_labels = num_labels\n",
        "#     self.bert = BertModel(config)\n",
        "#     self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "#     self.classifier = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
        "#                                     nn.ReLU(),\n",
        "#                                     nn.Linear(config.hidden_size, self.num_labels))\n",
        "#     for module in self.modules():\n",
        "#       if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "#               module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#       elif isinstance(module, BertLayerNorm):\n",
        "#           module.bias.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#           module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#       if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "#           module.bias.data.zero_()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDJ3PTVq70qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertSequence(nn.Module):\n",
        "  \n",
        "  def __init__(self, config, num_labels, layers):\n",
        "    \n",
        "    # layers:\n",
        "    # number of linear layers : 1 or 2\n",
        "    \n",
        "    # configs :\n",
        "    # bert-base-uncased: 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    # bert-large-uncased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    # bert-base-cased: 12-layer, 768-hidden, 12-heads , 110M parameters\n",
        "    # bert-large-cased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    \n",
        "    super(BertSequence, self).__init__()\n",
        "    \n",
        "    assert layers in [1,2]\n",
        "    \n",
        "    self.layers = layers\n",
        "    \n",
        "    self.hidden = 768\n",
        "    if self.layers == 1 :\n",
        "      self.hidden = num_labels\n",
        "    \n",
        "    self.num_labels = num_labels\n",
        "    self.config = config\n",
        "    self.bert = BertForSequenceClassification.from_pretrained(config, num_labels = self.hidden)\n",
        "    \n",
        "    if self.layers == 2:\n",
        "      self.relu = nn.ReLU()\n",
        "      self.layer = nn.Linear(self.hidden, self.num_labels)\n",
        "      self.layer.weight.data.normal_(mean=0.0, std=0.02)\n",
        "      self.layer.bias.data.zero_()\n",
        "    \n",
        "  def forward(self, input_ids, segment_ids, input_mask):\n",
        "    \n",
        "    out = self.bert(input_ids, segment_ids, input_mask)\n",
        "    \n",
        "    if self.layers == 2:\n",
        "      out = self.relu(out)\n",
        "      out = self.layer(out)\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joBV4vt8RKwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "gradient_accumulation_steps = 4\n",
        "num_train_epochs = 2000\n",
        "lr = 2e-2\n",
        "max_seq_length = 256\n",
        "warmup_proportion = 0.1 \n",
        "# Proportion of training to perform linear learning rate warmup for. \"\"E.g., 0.1 = 10% of training.\")\n",
        "lr_chng_iter = 3500\n",
        "eval_iter = 1000\n",
        "val_split = 0.1\n",
        "best_val = 0\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiDpKh2ARNGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "\n",
        "num_examples = 64000\n",
        "\n",
        "with open('data.tsv', \"r\", encoding='utf-8') as f:\n",
        "  reader = csv.reader(f, delimiter=\"\\t\")\n",
        "  for line in reader:\n",
        "    \n",
        "      num_examples-=1\n",
        "      if num_examples < 0:\n",
        "        break\n",
        "        \n",
        "      lines.append(line)\n",
        "      \n",
        "\n",
        "# Punctuation etc. stuff is handled by tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjIHavKmfTzs",
        "colab_type": "code",
        "outputId": "a1124a8c-c995-4798-d322-81fb61f9a440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "random.shuffle(lines)\n",
        "train_lines = lines[:int((1-val_split)*len(lines))]\n",
        "val_lines = lines[int((1-val_split)*len(lines)):]\n",
        "print('Train examples = ', len(train_lines))\n",
        "print('Val examples = ', len(val_lines))\n",
        "del lines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train examples =  57600\n",
            "Val examples =  6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VfHXcFXh79x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrMb4srqiG2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples = [InputExample(guid = line[0], text_a = line[1], text_b = line[2], label = line[3]) for line in train_lines]\n",
        "val_examples = [InputExample(guid = line[0], text_a = line[1], text_b = line[2], label = line[3]) for line in val_lines]\n",
        "del train_lines\n",
        "del val_lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCY6AxaMh3l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWRl2TrQJi11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXwUyEQijTlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFBsU60LcbY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        \n",
        "        if ex_index % 1000 == 0:\n",
        "          print('{} examples done out of {}'.format(ex_index, len(examples)))\n",
        "        \n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:7\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        label_id = label_map[int(example.label)]\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_id))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf5Ug5_RWPxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, labels):\n",
        "    outputs = np.argmax(out, axis=1)\n",
        "    \n",
        "#     correct_one = np.sum(np.where(outputs == 1)[0] == np.where(labels == 1)) #count of number of times the model predicted 1 and the label is also 1\n",
        "    correct_one = np.sum((outputs == labels) * outputs)\n",
        "    precision =  correct_one/np.sum(outputs == 1)\n",
        "    recall = correct_one/np.sum(labels==1)\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "#     print(\"f1: \", f1, \" precision:\", precision, \" recall:\", recall)\n",
        "#     print(\"labels:\", labels)\n",
        "#     print(\"out:\", out)\n",
        "    \n",
        "    return [np.sum(outputs == labels), f1, precision, recall, correct_one]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9va6BEdShXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hiO2xsrJi0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertSequence('bert-base-uncased', num_labels = 2, layers=1).cuda()\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "num_train_steps = int(len(train_examples)/batch_size) # steps in 1 epoch\n",
        "t_total = num_train_steps*num_train_epochs # total number of steps in training\n",
        "optimizer = BertAdam(optimizer_grouped_parameters, lr=lr, warmup=warmup_proportion, t_total=t_total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SudkAimBAVft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.backends.cudnn.benchmark = True    # would speed up runtime hopefully\n",
        "label_list = [0,1]         # label map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHbsjkLdJixD",
        "colab_type": "code",
        "outputId": "8e1b906b-47f1-4fc7-8b2b-8ec2d6cf36ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1003
        }
      },
      "source": [
        "train_features = convert_examples_to_features(train_examples, label_list, max_seq_length, tokenizer)\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "del train_features\n",
        "_ = model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 examples done out of 57600\n",
            "1000 examples done out of 57600\n",
            "2000 examples done out of 57600\n",
            "3000 examples done out of 57600\n",
            "4000 examples done out of 57600\n",
            "5000 examples done out of 57600\n",
            "6000 examples done out of 57600\n",
            "7000 examples done out of 57600\n",
            "8000 examples done out of 57600\n",
            "9000 examples done out of 57600\n",
            "10000 examples done out of 57600\n",
            "11000 examples done out of 57600\n",
            "12000 examples done out of 57600\n",
            "13000 examples done out of 57600\n",
            "14000 examples done out of 57600\n",
            "15000 examples done out of 57600\n",
            "16000 examples done out of 57600\n",
            "17000 examples done out of 57600\n",
            "18000 examples done out of 57600\n",
            "19000 examples done out of 57600\n",
            "20000 examples done out of 57600\n",
            "21000 examples done out of 57600\n",
            "22000 examples done out of 57600\n",
            "23000 examples done out of 57600\n",
            "24000 examples done out of 57600\n",
            "25000 examples done out of 57600\n",
            "26000 examples done out of 57600\n",
            "27000 examples done out of 57600\n",
            "28000 examples done out of 57600\n",
            "29000 examples done out of 57600\n",
            "30000 examples done out of 57600\n",
            "31000 examples done out of 57600\n",
            "32000 examples done out of 57600\n",
            "33000 examples done out of 57600\n",
            "34000 examples done out of 57600\n",
            "35000 examples done out of 57600\n",
            "36000 examples done out of 57600\n",
            "37000 examples done out of 57600\n",
            "38000 examples done out of 57600\n",
            "39000 examples done out of 57600\n",
            "40000 examples done out of 57600\n",
            "41000 examples done out of 57600\n",
            "42000 examples done out of 57600\n",
            "43000 examples done out of 57600\n",
            "44000 examples done out of 57600\n",
            "45000 examples done out of 57600\n",
            "46000 examples done out of 57600\n",
            "47000 examples done out of 57600\n",
            "48000 examples done out of 57600\n",
            "49000 examples done out of 57600\n",
            "50000 examples done out of 57600\n",
            "51000 examples done out of 57600\n",
            "52000 examples done out of 57600\n",
            "53000 examples done out of 57600\n",
            "54000 examples done out of 57600\n",
            "55000 examples done out of 57600\n",
            "56000 examples done out of 57600\n",
            "57000 examples done out of 57600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu4t5LefGn2L",
        "colab_type": "code",
        "outputId": "51cfc12c-ba23-43ba-ca2f-8163b408c96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "val_features = convert_examples_to_features(val_examples, label_list, max_seq_length, tokenizer)\n",
        "all_input_ids = torch.tensor([f.input_ids for f in val_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in val_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in val_features], dtype=torch.long)\n",
        "all_label_ids = torch.tensor([f.label_id for f in val_features], dtype=torch.long)\n",
        "val_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 examples done out of 6400\n",
            "1000 examples done out of 6400\n",
            "2000 examples done out of 6400\n",
            "3000 examples done out of 6400\n",
            "4000 examples done out of 6400\n",
            "5000 examples done out of 6400\n",
            "6000 examples done out of 6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioSYr1nEeZcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model(model, epoch, iteration, loss, label_list, max_seq_length, loss_fct):\n",
        "  \n",
        "  global best_val\n",
        "  global gradient_accumulation_steps\n",
        "  global file1 \n",
        "  \n",
        "  _ = model.eval()\n",
        "  \n",
        "  eval_loss, eval_accuracy, f1, precision, recall = 0, 0, 0, 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  outputs_all = np.array([])\n",
        "  label_ids_all = np.array([])\n",
        "  \n",
        "  for input_ids, input_mask, segment_ids, label_ids in val_dataloader:\n",
        "    input_ids = input_ids.cuda()\n",
        "    input_mask = input_mask.cuda()\n",
        "    segment_ids = segment_ids.cuda()\n",
        "    label_ids = label_ids.cuda()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, segment_ids, input_mask)\n",
        "    \n",
        "    outputs = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "    outputs_all = np.append(outputs_all, outputs)\n",
        "    label_ids_all = np.append(label_ids_all, label_ids.cpu().numpy())\n",
        "    \n",
        "    tmp_eval_loss = loss_fct(logits.view(-1, 2), label_ids.view(-1))\n",
        "    tmp_eval_loss /= gradient_accumulation_steps\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    \n",
        "    nb_eval_examples += input_ids.size(0)\n",
        "    nb_eval_steps += 1\n",
        "    \n",
        "  eval_loss = eval_loss / nb_eval_steps\n",
        "  \n",
        "  correct_one = np.sum((outputs_all == label_ids_all) * outputs_all)\n",
        "  precision =  correct_one/np.sum(outputs_all)\n",
        "  recall = correct_one/np.sum(label_ids_all)\n",
        "  f1 = 2*precision*recall/(precision+recall)\n",
        "  \n",
        "  print('total 1s in output = ', np.sum(outputs_all))\n",
        "  print('total 1s in labels = ', np.sum(label_ids_all))\n",
        "  print('f1 = ', f1)\n",
        "  \n",
        "  if np.isnan(f1):\n",
        "    f1 = 0\n",
        "  \n",
        "  if f1 >= best_val:\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "    output_model_file = \"best_val.bin\"\n",
        "    torch.save({'stat_dict': model_to_save.state_dict(), \n",
        "                'eval_loss' : eval_loss,\n",
        "                'eval_accuracy' : eval_accuracy,\n",
        "                'f1' : f1,\n",
        "                'precision' : precision,\n",
        "                'recall' : recall}\n",
        "                , output_model_file)\n",
        "    best_val = f1\n",
        "\n",
        "    file1.SetContentFile('best_val.bin')\n",
        "    file1.Upload()\n",
        "    print('Best f1 = ', f1, ' precision = ', precision, ' recall = ', recall, ' loss = ', eval_loss)\n",
        "      \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1afdadb9-ba50-47b6-d687-6ea9d261b684",
        "id": "2120f2qeGFiA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6395
        }
      },
      "source": [
        "weights = [0.1, 0.9]\n",
        "class_weights = torch.FloatTensor(weights).cuda()\n",
        "loss_fct = CrossEntropyLoss(weight = class_weights)\n",
        "\n",
        "total_step = 0\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "  print('epoch = ', epoch)\n",
        "  for iteration, batch in enumerate(train_dataloader):\n",
        "    input_ids, input_mask, segment_ids, label_ids = batch\n",
        "    input_ids = input_ids.cuda()\n",
        "    input_mask = input_mask.cuda()\n",
        "    segment_ids = segment_ids.cuda()\n",
        "    label_ids = label_ids.cuda()\n",
        "    \n",
        "    logits = model(input_ids, segment_ids, input_mask)\n",
        "    \n",
        "    train_accuracy_params = accuracy(logits.detach().cpu().numpy(), label_ids.cpu().numpy())\n",
        "    train_accuracy = train_accuracy_params[0]\n",
        "    f1 = train_accuracy_params[1]\n",
        "    precision = train_accuracy_params[2]\n",
        "    recall = train_accuracy_params[3]\n",
        "    correct_one = train_accuracy_params[4]\n",
        "    \n",
        "    loss = loss_fct(logits.view(-1, 2), label_ids.view(-1))\n",
        "    loss = loss / gradient_accumulation_steps\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    if (iteration + 1) % gradient_accumulation_steps == 0:\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "    if iteration % lr_chng_iter == 0:\n",
        "      new_lr = lr * warmup_linear(total_step/t_total, warmup_proportion)\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = new_lr\n",
        "    \n",
        "    total_step += 1\n",
        "    \n",
        "    if iteration % 10 == 0:\n",
        "      print(\"iteration:\", iteration, \" loss:\", loss.item(), \"train_accuracy:\", train_accuracy, \" f1:\", f1, \" precision:\", precision, \" recall:\", recall, \" correct_one:\", correct_one)\n",
        "    \n",
        "    if (iteration + 1) % eval_iter == 0:\n",
        "        eval_model(model, epoch, iteration, loss, label_list, max_seq_length, loss_fct)\n",
        "        _ = model.train()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch =  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration: 0  loss: 0.17802616953849792 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration: 10  loss: 0.1949089616537094 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 20  loss: 0.16716812551021576 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 30  loss: 0.14277902245521545 train_accuracy: 16  f1: 1.0  precision: 1.0  recall: 1.0  correct_one: 1\n",
            "iteration: 40  loss: 0.14504100382328033 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 50  loss: 0.13729235529899597 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 60  loss: 0.18230295181274414 train_accuracy: 14  f1: 0.5  precision: 1.0  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 70  loss: 0.18294058740139008 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 80  loss: 0.1831817924976349 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 90  loss: 0.20220918953418732 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 100  loss: 0.13364899158477783 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 110  loss: 0.19147954881191254 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 120  loss: 0.1783851981163025 train_accuracy: 11  f1: 0.28571428571428575  precision: 0.25  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 130  loss: 0.18508554995059967 train_accuracy: 12  f1: 0.3333333333333333  precision: 0.3333333333333333  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 140  loss: 0.1813027262687683 train_accuracy: 13  f1: 0.4  precision: 0.5  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 150  loss: 0.1601685881614685 train_accuracy: 13  f1: 0.4  precision: 0.3333333333333333  recall: 0.5  correct_one: 1\n",
            "iteration: 160  loss: 0.1886167824268341 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 170  loss: 0.13213318586349487 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 180  loss: 0.17843227088451385 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 190  loss: 0.1798645704984665 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 200  loss: 0.1797533482313156 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 210  loss: 0.16898919641971588 train_accuracy: 14  f1: 0.5  precision: 0.5  recall: 0.5  correct_one: 1\n",
            "iteration: 220  loss: 0.1897049993276596 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 230  loss: 0.1726604551076889 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 240  loss: 0.1693720817565918 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 250  loss: 0.1723148077726364 train_accuracy: 14  f1: 0.5  precision: 0.5  recall: 0.5  correct_one: 1\n",
            "iteration: 260  loss: 0.15506500005722046 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 270  loss: 0.18959079682826996 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 280  loss: 0.1941395252943039 train_accuracy: 9  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 290  loss: 0.174319326877594 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 300  loss: 0.2053532898426056 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 310  loss: 0.16795405745506287 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 320  loss: 0.15280331671237946 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 330  loss: 0.15456943213939667 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 340  loss: 0.2001146376132965 train_accuracy: 13  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 350  loss: 0.14453785121440887 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 360  loss: 0.19874827563762665 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 370  loss: 0.13783371448516846 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 380  loss: 0.188735693693161 train_accuracy: 12  f1: 0.3333333333333333  precision: 0.3333333333333333  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 390  loss: 0.20806752145290375 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 400  loss: 0.18345674872398376 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 410  loss: 0.1749686747789383 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 420  loss: 0.19939030706882477 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 430  loss: 0.19510655105113983 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 440  loss: 0.16641373932361603 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 450  loss: 0.18210481107234955 train_accuracy: 13  f1: 0.4  precision: 0.5  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 460  loss: 0.1819216012954712 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 470  loss: 0.15164048969745636 train_accuracy: 14  f1: 0.5  precision: 0.3333333333333333  recall: 1.0  correct_one: 1\n",
            "iteration: 480  loss: 0.19548027217388153 train_accuracy: 11  f1: 0.28571428571428575  precision: 0.25  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 490  loss: 0.16539667546749115 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 500  loss: 0.17150788009166718 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 510  loss: 0.18162542581558228 train_accuracy: 12  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 520  loss: 0.1885833889245987 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 530  loss: 0.18866601586341858 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 540  loss: 0.16256442666053772 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 550  loss: 0.19335779547691345 train_accuracy: 9  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 560  loss: 0.17124778032302856 train_accuracy: 13  f1: 0.4  precision: 0.3333333333333333  recall: 0.5  correct_one: 1\n",
            "iteration: 570  loss: 0.18708473443984985 train_accuracy: 13  f1: 0.4  precision: 1.0  recall: 0.25  correct_one: 1\n",
            "iteration: 580  loss: 0.17894364893436432 train_accuracy: 15  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 590  loss: 0.15218374133110046 train_accuracy: 15  f1: 0.6666666666666666  precision: 0.5  recall: 1.0  correct_one: 1\n",
            "iteration: 600  loss: 0.13155537843704224 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 610  loss: 0.17908214032649994 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 620  loss: 0.19936105608940125 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 630  loss: 0.1930171698331833 train_accuracy: 15  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 640  loss: 0.18169757723808289 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 650  loss: 0.17093122005462646 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 660  loss: 0.1490975320339203 train_accuracy: 15  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 670  loss: 0.18388064205646515 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 680  loss: 0.16088324785232544 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 690  loss: 0.20097938179969788 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 700  loss: 0.18741166591644287 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 710  loss: 0.17057208716869354 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 720  loss: 0.15168224275112152 train_accuracy: 12  f1: 0.33333333333333337  precision: 0.2  recall: 1.0  correct_one: 1\n",
            "iteration: 730  loss: 0.1848267763853073 train_accuracy: 9  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 740  loss: 0.17201057076454163 train_accuracy: 13  f1: 0.4  precision: 0.3333333333333333  recall: 0.5  correct_one: 1\n",
            "iteration: 750  loss: 0.18564768135547638 train_accuracy: 13  f1: 0.4  precision: 0.5  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 760  loss: 0.1600273996591568 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 770  loss: 0.18870605528354645 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 780  loss: 0.15830032527446747 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 790  loss: 0.16072440147399902 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 800  loss: 0.14756925404071808 train_accuracy: 15  f1: 0.6666666666666666  precision: 0.5  recall: 1.0  correct_one: 1\n",
            "iteration: 810  loss: 0.18004652857780457 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 820  loss: 0.19131483137607574 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 830  loss: 0.17514337599277496 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 840  loss: 0.13482503592967987 train_accuracy: 16  f1: nan  precision: nan  recall: nan  correct_one: 0\n",
            "iteration: 850  loss: 0.17406049370765686 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 860  loss: 0.2104322463274002 train_accuracy: 13  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 870  loss: 0.17655989527702332 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 880  loss: 0.21002992987632751 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 890  loss: 0.1854746788740158 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 900  loss: 0.16614171862602234 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 910  loss: 0.185821533203125 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 920  loss: 0.19603927433490753 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 930  loss: 0.16506540775299072 train_accuracy: 13  f1: 0.4  precision: 0.3333333333333333  recall: 0.5  correct_one: 1\n",
            "iteration: 940  loss: 0.1737254559993744 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 950  loss: 0.18698523938655853 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 960  loss: 0.18353314697742462 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 970  loss: 0.16583329439163208 train_accuracy: 13  f1: 0.4  precision: 0.3333333333333333  recall: 0.5  correct_one: 1\n",
            "iteration: 980  loss: 0.1776503473520279 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 990  loss: 0.17538262903690338 train_accuracy: 14  f1: 0.5  precision: 0.5  recall: 0.5  correct_one: 1\n",
            "total 1s in output =  59.0\n",
            "total 1s in labels =  646.0\n",
            "f1 =  0.005673758865248227\n",
            "Best f1 =  0.005673758865248227  precision =  0.03389830508474576  recall =  0.0030959752321981426  loss =  0.17256345342844726\n",
            "iteration: 1000  loss: 0.19530731439590454 train_accuracy: 13  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1010  loss: 0.1790781170129776 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1020  loss: 0.22849373519420624 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1030  loss: 0.15907874703407288 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1040  loss: 0.13126294314861298 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1050  loss: 0.15316343307495117 train_accuracy: 15  f1: 0.6666666666666666  precision: 0.5  recall: 1.0  correct_one: 1\n",
            "iteration: 1060  loss: 0.1640697419643402 train_accuracy: 13  f1: 0.4  precision: 0.3333333333333333  recall: 0.5  correct_one: 1\n",
            "iteration: 1070  loss: 0.1895420253276825 train_accuracy: 14  f1: 0.5  precision: 1.0  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 1080  loss: 0.16783688962459564 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1090  loss: 0.18843939900398254 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1100  loss: 0.17820744216442108 train_accuracy: 13  f1: 0.4  precision: 0.5  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 1110  loss: 0.1924208700656891 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1120  loss: 0.20025579631328583 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1130  loss: 0.17588035762310028 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1140  loss: 0.14405158162117004 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1150  loss: 0.21902701258659363 train_accuracy: 12  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1160  loss: 0.18702812492847443 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1170  loss: 0.18299050629138947 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1180  loss: 0.1819155216217041 train_accuracy: 13  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1190  loss: 0.1688501536846161 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1200  loss: 0.19486230611801147 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1210  loss: 0.13705886900424957 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1220  loss: 0.17708434164524078 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1230  loss: 0.18905127048492432 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1240  loss: 0.1861588954925537 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1250  loss: 0.135854110121727 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1260  loss: 0.2092779278755188 train_accuracy: 11  f1: 0.28571428571428575  precision: 0.3333333333333333  recall: 0.25  correct_one: 1\n",
            "iteration: 1270  loss: 0.18621602654457092 train_accuracy: 15  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1280  loss: 0.1526612937450409 train_accuracy: 16  f1: 1.0  precision: 1.0  recall: 1.0  correct_one: 1\n",
            "iteration: 1290  loss: 0.17035353183746338 train_accuracy: 15  f1: 0.6666666666666666  precision: 1.0  recall: 0.5  correct_one: 1\n",
            "iteration: 1300  loss: 0.14220620691776276 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1310  loss: 0.1622379720211029 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1320  loss: 0.2070174217224121 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1330  loss: 0.18907098472118378 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1340  loss: 0.14996546506881714 train_accuracy: 16  f1: 1.0  precision: 1.0  recall: 1.0  correct_one: 1\n",
            "iteration: 1350  loss: 0.18079836666584015 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1360  loss: 0.18846598267555237 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1370  loss: 0.18771019577980042 train_accuracy: 15  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1380  loss: 0.20266564190387726 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1390  loss: 0.1841653436422348 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1400  loss: 0.16831016540527344 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1410  loss: 0.19262713193893433 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1420  loss: 0.14404402673244476 train_accuracy: 12  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1430  loss: 0.1813572645187378 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1440  loss: 0.13167805969715118 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1450  loss: 0.15962348878383636 train_accuracy: 13  f1: 0.4  precision: 0.3333333333333333  recall: 0.5  correct_one: 1\n",
            "iteration: 1460  loss: 0.17172428965568542 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1470  loss: 0.1899951696395874 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1480  loss: 0.1505863070487976 train_accuracy: 15  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1490  loss: 0.18613846600055695 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1500  loss: 0.16745123267173767 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1510  loss: 0.18383410573005676 train_accuracy: 15  f1: 0.6666666666666666  precision: 1.0  recall: 0.5  correct_one: 1\n",
            "iteration: 1520  loss: 0.17576593160629272 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1530  loss: 0.17620711028575897 train_accuracy: 15  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1540  loss: 0.15394191443920135 train_accuracy: 12  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1550  loss: 0.18177811801433563 train_accuracy: 14  f1: 0.5  precision: 1.0  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 1560  loss: 0.18680037558078766 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1570  loss: 0.19345757365226746 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1580  loss: 0.13616159558296204 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1590  loss: 0.19718147814273834 train_accuracy: 13  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1600  loss: 0.20369257032871246 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1610  loss: 0.16001637279987335 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1620  loss: 0.1938801407814026 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1630  loss: 0.21206404268741608 train_accuracy: 9  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1640  loss: 0.1850237399339676 train_accuracy: 13  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1650  loss: 0.198308527469635 train_accuracy: 9  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1660  loss: 0.16653430461883545 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1670  loss: 0.15727630257606506 train_accuracy: 15  f1: 0.8  precision: 0.6666666666666666  recall: 1.0  correct_one: 2\n",
            "iteration: 1680  loss: 0.18301132321357727 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1690  loss: 0.19314460456371307 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1700  loss: 0.1803234964609146 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1710  loss: 0.17227208614349365 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1720  loss: 0.18678122758865356 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1730  loss: 0.20391345024108887 train_accuracy: 12  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1740  loss: 0.15965992212295532 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1750  loss: 0.20334111154079437 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1760  loss: 0.18000152707099915 train_accuracy: 11  f1: 0.28571428571428575  precision: 0.25  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 1770  loss: 0.17482082545757294 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1780  loss: 0.185052752494812 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1790  loss: 0.16007769107818604 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1800  loss: 0.16872411966323853 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1810  loss: 0.1821712702512741 train_accuracy: 12  f1: 0.3333333333333333  precision: 0.25  recall: 0.5  correct_one: 1\n",
            "iteration: 1820  loss: 0.1474609375 train_accuracy: 12  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1830  loss: 0.1660837084054947 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1840  loss: 0.17956507205963135 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1850  loss: 0.14183272421360016 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1860  loss: 0.19196636974811554 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1870  loss: 0.1460471898317337 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1880  loss: 0.15107113122940063 train_accuracy: 12  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1890  loss: 0.13524818420410156 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1900  loss: 0.16945168375968933 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1910  loss: 0.17459264397621155 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1920  loss: 0.2093597650527954 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 1930  loss: 0.1356595903635025 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1940  loss: 0.17336390912532806 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1950  loss: 0.14216731488704681 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 1960  loss: 0.19044286012649536 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1970  loss: 0.18979914486408234 train_accuracy: 10  f1: 0.25  precision: 0.25  recall: 0.25  correct_one: 1\n",
            "iteration: 1980  loss: 0.19370004534721375 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 1990  loss: 0.14070530235767365 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "total 1s in output =  59.0\n",
            "total 1s in labels =  646.0\n",
            "f1 =  0.005673758865248227\n",
            "Best f1 =  0.005673758865248227  precision =  0.03389830508474576  recall =  0.0030959752321981426  loss =  0.17297081012278795\n",
            "iteration: 2000  loss: 0.21790096163749695 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2010  loss: 0.13919298350811005 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2020  loss: 0.18889722228050232 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2030  loss: 0.17346462607383728 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2040  loss: 0.1737227439880371 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2050  loss: 0.1662365198135376 train_accuracy: 14  f1: 0.5  precision: 0.5  recall: 0.5  correct_one: 1\n",
            "iteration: 2060  loss: 0.15773813426494598 train_accuracy: 14  f1: 0.5  precision: 0.5  recall: 0.5  correct_one: 1\n",
            "iteration: 2070  loss: 0.14039792120456696 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2080  loss: 0.19996106624603271 train_accuracy: 9  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2090  loss: 0.1701442450284958 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2100  loss: 0.215040922164917 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2110  loss: 0.16522718966007233 train_accuracy: 14  f1: 0.5  precision: 0.5  recall: 0.5  correct_one: 1\n",
            "iteration: 2120  loss: 0.13099850714206696 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2130  loss: 0.14111287891864777 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2140  loss: 0.1745128482580185 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2150  loss: 0.1444687694311142 train_accuracy: 16  f1: nan  precision: nan  recall: nan  correct_one: 0\n",
            "iteration: 2160  loss: 0.13704487681388855 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2170  loss: 0.14184240996837616 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2180  loss: 0.20319338142871857 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2190  loss: 0.15751492977142334 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2200  loss: 0.1419646441936493 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2210  loss: 0.15338154137134552 train_accuracy: 13  f1: 0.4  precision: 0.25  recall: 1.0  correct_one: 1\n",
            "iteration: 2220  loss: 0.1848459094762802 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2230  loss: 0.18288159370422363 train_accuracy: 14  f1: 0.5  precision: 1.0  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 2240  loss: 0.12690356373786926 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2250  loss: 0.17818568646907806 train_accuracy: 14  f1: 0.5  precision: 1.0  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 2260  loss: 0.1801399439573288 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2270  loss: 0.18130700290203094 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2280  loss: 0.1973983496427536 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2290  loss: 0.1776939183473587 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2300  loss: 0.20145469903945923 train_accuracy: 10  f1: 0.25  precision: 0.5  recall: 0.16666666666666666  correct_one: 1\n",
            "iteration: 2310  loss: 0.13761062920093536 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2320  loss: 0.18709640204906464 train_accuracy: 14  f1: 0.5  precision: 1.0  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 2330  loss: 0.1443057358264923 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2340  loss: 0.13441328704357147 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2350  loss: 0.16661638021469116 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2360  loss: 0.1917477250099182 train_accuracy: 13  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 2370  loss: 0.19592620432376862 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2380  loss: 0.1618228554725647 train_accuracy: 15  f1: 0.6666666666666666  precision: 1.0  recall: 0.5  correct_one: 1\n",
            "iteration: 2390  loss: 0.16333171725273132 train_accuracy: 15  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 2400  loss: 0.1464059054851532 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2410  loss: 0.14217613637447357 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2420  loss: 0.1819067895412445 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2430  loss: 0.1480821818113327 train_accuracy: 14  f1: 0.5  precision: 0.3333333333333333  recall: 1.0  correct_one: 1\n",
            "iteration: 2440  loss: 0.17712469398975372 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2450  loss: 0.18114811182022095 train_accuracy: 15  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 2460  loss: 0.20227280259132385 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2470  loss: 0.20301391184329987 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2480  loss: 0.18179616332054138 train_accuracy: 14  f1: 0.5  precision: 1.0  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 2490  loss: 0.14375150203704834 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2500  loss: 0.20244088768959045 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2510  loss: 0.17439685761928558 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2520  loss: 0.14901429414749146 train_accuracy: 14  f1: 0.5  precision: 0.3333333333333333  recall: 1.0  correct_one: 1\n",
            "iteration: 2530  loss: 0.19568006694316864 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2540  loss: 0.17580083012580872 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2550  loss: 0.21310080587863922 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2560  loss: 0.17422731220722198 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2570  loss: 0.17332057654857635 train_accuracy: 15  f1: 0.6666666666666666  precision: 1.0  recall: 0.5  correct_one: 1\n",
            "iteration: 2580  loss: 0.1759268343448639 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2590  loss: 0.14745359122753143 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2600  loss: 0.15484893321990967 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2610  loss: 0.16926218569278717 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2620  loss: 0.14497877657413483 train_accuracy: 15  f1: 0.6666666666666666  precision: 0.5  recall: 1.0  correct_one: 1\n",
            "iteration: 2630  loss: 0.18172386288642883 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2640  loss: 0.155581533908844 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2650  loss: 0.16510827839374542 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2660  loss: 0.16345176100730896 train_accuracy: 15  f1: 0.6666666666666666  precision: 1.0  recall: 0.5  correct_one: 1\n",
            "iteration: 2670  loss: 0.1598268449306488 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2680  loss: 0.19733403623104095 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2690  loss: 0.22821718454360962 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2700  loss: 0.181224063038826 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 2710  loss: 0.1647159457206726 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2720  loss: 0.19219042360782623 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2730  loss: 0.1663598269224167 train_accuracy: 11  f1: 0.28571428571428575  precision: 0.2  recall: 0.5  correct_one: 1\n",
            "iteration: 2740  loss: 0.15109135210514069 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2750  loss: 0.18069219589233398 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2760  loss: 0.1749950498342514 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2770  loss: 0.12841306626796722 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2780  loss: 0.17805103957653046 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 2790  loss: 0.16095884144306183 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2800  loss: 0.17309395968914032 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2810  loss: 0.19495323300361633 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2820  loss: 0.1478056013584137 train_accuracy: 14  f1: 0.5  precision: 0.3333333333333333  recall: 1.0  correct_one: 1\n",
            "iteration: 2830  loss: 0.19551916420459747 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2840  loss: 0.17225967347621918 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2850  loss: 0.18559902906417847 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2860  loss: 0.18768839538097382 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2870  loss: 0.15738451480865479 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2880  loss: 0.13998232781887054 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2890  loss: 0.21314385533332825 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2900  loss: 0.21406124532222748 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2910  loss: 0.1997508704662323 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2920  loss: 0.1602230817079544 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2930  loss: 0.14360646903514862 train_accuracy: 16  f1: 1.0  precision: 1.0  recall: 1.0  correct_one: 1\n",
            "iteration: 2940  loss: 0.18852072954177856 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 2950  loss: 0.18719275295734406 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 2960  loss: 0.14499691128730774 train_accuracy: 12  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2970  loss: 0.163070946931839 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 2980  loss: 0.1536252498626709 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 2990  loss: 0.17930985987186432 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "total 1s in output =  59.0\n",
            "total 1s in labels =  646.0\n",
            "f1 =  0.005673758865248227\n",
            "Best f1 =  0.005673758865248227  precision =  0.03389830508474576  recall =  0.0030959752321981426  loss =  0.1723973260819912\n",
            "iteration: 3000  loss: 0.1767217218875885 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3010  loss: 0.20249275863170624 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3020  loss: 0.18828752636909485 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3030  loss: 0.1876184493303299 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3040  loss: 0.14289306104183197 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 3050  loss: 0.17566019296646118 train_accuracy: 13  f1: 0.4  precision: 0.5  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 3060  loss: 0.18114127218723297 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3070  loss: 0.20032356679439545 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3080  loss: 0.1702856868505478 train_accuracy: 14  f1: 0.5  precision: 0.5  recall: 0.5  correct_one: 1\n",
            "iteration: 3090  loss: 0.21410615742206573 train_accuracy: 8  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3100  loss: 0.22651183605194092 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3110  loss: 0.1535799503326416 train_accuracy: 13  f1: 0.4  precision: 0.3333333333333333  recall: 0.5  correct_one: 1\n",
            "iteration: 3120  loss: 0.13830450177192688 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 3130  loss: 0.180425763130188 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3140  loss: 0.2020946592092514 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3150  loss: 0.18442609906196594 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3160  loss: 0.1481398344039917 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 3170  loss: 0.14729167520999908 train_accuracy: 13  f1: 0.4  precision: 0.25  recall: 1.0  correct_one: 1\n",
            "iteration: 3180  loss: 0.18509842455387115 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3190  loss: 0.20674258470535278 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3200  loss: 0.2057836800813675 train_accuracy: 13  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 3210  loss: 0.21743865311145782 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3220  loss: 0.1497577726840973 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3230  loss: 0.1339341700077057 train_accuracy: 15  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 3240  loss: 0.1967182159423828 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3250  loss: 0.17377176880836487 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3260  loss: 0.15477316081523895 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3270  loss: 0.19778360426425934 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 3280  loss: 0.1758316308259964 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3290  loss: 0.20788373053073883 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3300  loss: 0.17524529993534088 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3310  loss: 0.2040160745382309 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3320  loss: 0.16882766783237457 train_accuracy: 14  f1: 0.5  precision: 1.0  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 3330  loss: 0.1473669409751892 train_accuracy: 14  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 3340  loss: 0.18625691533088684 train_accuracy: 12  f1: 0.3333333333333333  precision: 0.3333333333333333  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 3350  loss: 0.1692972630262375 train_accuracy: 13  f1: 0.4  precision: 0.5  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 3360  loss: 0.19585908949375153 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3370  loss: 0.1935066431760788 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3380  loss: 0.17343659698963165 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3390  loss: 0.15699566900730133 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3400  loss: 0.17904691398143768 train_accuracy: 14  f1: 0.5  precision: 0.5  recall: 0.5  correct_one: 1\n",
            "iteration: 3410  loss: 0.1468271166086197 train_accuracy: 15  f1: 0.6666666666666666  precision: 0.5  recall: 1.0  correct_one: 1\n",
            "iteration: 3420  loss: 0.17424048483371735 train_accuracy: 11  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3430  loss: 0.17223836481571198 train_accuracy: 15  f1: 0.6666666666666666  precision: 1.0  recall: 0.5  correct_one: 1\n",
            "iteration: 3440  loss: 0.17020763456821442 train_accuracy: 13  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3450  loss: 0.18195675313472748 train_accuracy: 14  f1: nan  precision: nan  recall: 0.0  correct_one: 0\n",
            "iteration: 3460  loss: 0.14850158989429474 train_accuracy: 13  f1: nan  precision: 0.0  recall: nan  correct_one: 0\n",
            "iteration: 3470  loss: 0.15194882452487946 train_accuracy: 12  f1: 0.3333333333333333  precision: 0.25  recall: 0.5  correct_one: 1\n",
            "iteration: 3480  loss: 0.17398181557655334 train_accuracy: 14  f1: 0.5  precision: 1.0  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 3490  loss: 0.19112925231456757 train_accuracy: 9  f1: 0.22222222222222224  precision: 0.25  recall: 0.2  correct_one: 1\n",
            "iteration: 3500  loss: 0.1813700795173645 train_accuracy: 12  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3510  loss: 0.20877771079540253 train_accuracy: 12  f1: 0.3333333333333333  precision: 0.3333333333333333  recall: 0.3333333333333333  correct_one: 1\n",
            "iteration: 3520  loss: 0.15948624908924103 train_accuracy: 14  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3530  loss: 0.17401638627052307 train_accuracy: 10  f1: nan  precision: 0.0  recall: 0.0  correct_one: 0\n",
            "iteration: 3540  loss: 0.17392267286777496 train_accuracy: 14  f1: 0.5  precision: 0.5  recall: 0.5  correct_one: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDKErR_WBQ3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K1doqHifBUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MORBPO3pfBdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjM7CejsfHaY",
        "colab_type": "text"
      },
      "source": [
        "# **Make Prediction on Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDogdP2AfZDi",
        "colab": {}
      },
      "source": [
        "# # Run this only once on a machine\n",
        "\n",
        "# !pip install pytorch-pretrained-bert\n",
        "# !pip install livelossplot\n",
        "# !wget \"https://competitions.codalab.org/my/datasets/download/69a3e8d0-b836-48b8-8795-36a6865a1c04\"\n",
        "# !unzip 69a3e8d0-b836-48b8-8795-36a6865a1c04\n",
        "# !rm 69a3e8d0-b836-48b8-8795-36a6865a1c04\n",
        "# !ls -lh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uXZiO5cMfZDt",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "63efde02-a6c2-4bb5-c95c-4fb816a3bbba",
        "id": "uUW83tTTj6Kh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn as nn\n",
        "from livelossplot import PlotLosses\n",
        "import random\n",
        "import csv\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPG9PnM7fDMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = drive.CreateFile({'id': '1ksD6TjCMGPShqMuQa3hMaKReURYZdr_V'})\n",
        "file.GetContentFile('best_val.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hguBq0POgIM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertSequence(nn.Module):\n",
        "  \n",
        "  def __init__(self, config, num_labels, layers):\n",
        "    \n",
        "    # layers:\n",
        "    # number of linear layers : 1 or 2\n",
        "    \n",
        "    # configs :\n",
        "    # bert-base-uncased: 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    # bert-large-uncased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    # bert-base-cased: 12-layer, 768-hidden, 12-heads , 110M parameters\n",
        "    # bert-large-cased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    \n",
        "    super(BertSequence, self).__init__()\n",
        "    \n",
        "    assert layers in [1,2]\n",
        "    \n",
        "    self.layers = layers\n",
        "    \n",
        "    self.hidden = 768\n",
        "    if self.layers == 1 :\n",
        "      self.hidden = num_labels\n",
        "    \n",
        "    self.num_labels = num_labels\n",
        "    self.config = config\n",
        "    self.bert = BertForSequenceClassification.from_pretrained(config, num_labels = self.hidden)\n",
        "    \n",
        "    if self.layers == 2:\n",
        "      self.relu = nn.ReLU()\n",
        "      self.layer = nn.Linear(self.hidden, self.num_labels)\n",
        "      self.layer.weight.data.normal_(mean=0.0, std=0.02)\n",
        "      self.layer.bias.data.zero_()\n",
        "    \n",
        "  def forward(self, input_ids, segment_ids, input_mask):\n",
        "    \n",
        "    out = self.bert(input_ids, segment_ids, input_mask)\n",
        "    \n",
        "    if self.layers == 2:\n",
        "      out = self.relu(out)\n",
        "      out = self.layer(out)\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrM3oKg4i0Ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertSequence('bert-base-uncased', num_labels = 2, layers=2).cuda()\n",
        "modelCheckpoint = torch.load(\"best_val.bin\", map_location=lambda storage, loc: storage)\n",
        "model.load_state_dict(modelCheckpoint['stat_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97vPwdWejUBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "num_examples = 10500\n",
        "\n",
        "with open('eval1_unlabelled.tsv', \"r\", encoding='utf-8') as f:\n",
        "  reader = csv.reader(f, delimiter=\"\\t\")\n",
        "  for line in reader:\n",
        "    num_examples-=1\n",
        "    if num_examples < 0:\n",
        "      break\n",
        "\n",
        "    lines.append(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J90HO86AlK_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiOAu4-akTXg",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, id_):\n",
        "        self.id_ = id_\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dhWtSeBskTXq",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vn7tvsmpkTXw",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "30vcX8eYkTX8",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        \n",
        "        if ex_index % 1000 == 0:\n",
        "          print('{} examples done out of {}'.format(ex_index, len(examples)))\n",
        "        \n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:7\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                             id_ = example.guid))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eckwwIv-iz-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_examples = [InputExample(guid = line[0], text_a = line[1], text_b = line[2]) for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkAvx2pElQqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_list = [0,1]\n",
        "batch_size = 10\n",
        "max_seq_length = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuTUIpdGsREn",
        "colab_type": "code",
        "outputId": "56d30a49-b097-45e4-fe9d-9e58e3640194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "val_features = convert_examples_to_features(val_examples, max_seq_length, tokenizer)\n",
        "all_input_ids = torch.tensor([f.input_ids for f in val_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in val_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in val_features], dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 examples done out of 10500\n",
            "1000 examples done out of 10500\n",
            "2000 examples done out of 10500\n",
            "3000 examples done out of 10500\n",
            "4000 examples done out of 10500\n",
            "5000 examples done out of 10500\n",
            "6000 examples done out of 10500\n",
            "7000 examples done out of 10500\n",
            "8000 examples done out of 10500\n",
            "9000 examples done out of 10500\n",
            "10000 examples done out of 10500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x6cTVoJRk4gB",
        "colab": {}
      },
      "source": [
        "all_ids = torch.tensor([int(f.id_) for f in val_features], dtype=torch.long)\n",
        "val_data = TensorDataset(all_ids, all_input_ids, all_input_mask, all_segment_ids)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1wS37zJpCdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJieHnfglNA_",
        "colab_type": "code",
        "outputId": "af5ca3b0-44bf-4044-f7ce-5713460799b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "file = open('answers.tsv', 'w')\n",
        "i = 0\n",
        "for all_ids, input_ids, input_mask, segment_ids in val_dataloader:\n",
        "  i+=1\n",
        "  if i% 100 == 0:\n",
        "    print('Done ', i)\n",
        "  input_ids = input_ids.cuda()\n",
        "  input_mask = input_mask.cuda()\n",
        "  segment_ids = segment_ids.cuda()\n",
        "\n",
        "  logits = model(input_ids, segment_ids, input_mask)\n",
        "  score = logits[:,1].detach().cpu().numpy()  \n",
        "  file.write('{}\\t'.format(all_ids[0].item()))\n",
        "  for x in score[:-1]:\n",
        "    file.write('{}\\t'.format(x))\n",
        "  file.write('{}\\n'.format(score[-1]))  \n",
        "  \n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done  100\n",
            "Done  200\n",
            "Done  300\n",
            "Done  400\n",
            "Done  500\n",
            "Done  600\n",
            "Done  700\n",
            "Done  800\n",
            "Done  900\n",
            "Done  1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fu7Sb5sqjxx",
        "colab_type": "code",
        "outputId": "38af2dd1-27db-4744-c74e-438cd6aa4fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_ids.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsDVbHIrqDL5",
        "colab": {}
      },
      "source": [
        "file1 = drive.CreateFile({'title': 'answers.tsv'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMQ2jj5klNnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1.SetContentFile('answers.tsv')\n",
        "file1.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyXCKoAwvidr",
        "colab_type": "code",
        "outputId": "36f81686-accf-4374-bf8f-d13eb509c711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json     best_val.bin  eval1_unlabelled.tsv\n",
            "answers.tsv  data.tsv\t   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYL0V5AllNqM",
        "colab_type": "code",
        "outputId": "819de957-532c-4d2b-c605-6baca5ce6114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# linse = []\n",
        "# with open('answers.tsv', 'r') as f:\n",
        "#   reader = csv.reader(f, delimiter=\"\\t\")\n",
        "#   for line in reader:\n",
        "#       linse.append(line)\n",
        "# linse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['1135787',\n",
              "  '-0.1890496015548706',\n",
              "  '-0.20414724946022034',\n",
              "  '-0.20256561040878296',\n",
              "  '-0.19391247630119324',\n",
              "  '-0.1895381510257721',\n",
              "  '-0.19488048553466797',\n",
              "  '-0.1877790093421936',\n",
              "  '-0.20241910219192505',\n",
              "  '-0.20091071724891663',\n",
              "  '-0.20497986674308777']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}